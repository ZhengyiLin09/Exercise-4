---
title: "ECO395M Exercises 04"
author: "Zhengyi Lin"
date: "4/13/2023"
output:
  md_document:
    variant: markdown_github

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, dpi = 300)
library(tidyverse)
library(knitr)
library(reshape2)
library(foreach)
library(gridExtra)
library(mosaic)
library(LICORS)
library(arules) 
library(arulesViz)
library(igraph)
theme_set(theme_minimal())
```


## Question 1: Clustering and PCA


### Clustering using K-means++

We used K mean++ initialization to divide the data into 2 different clusters, as we tried to find out if the unsupervised algorithm could distinguish red from white by looking at 11 chemicals.

```{r clustering, echo=FALSE}
wine <- read.csv("wine.csv")
### data cleaning 
# Center and scale the data
X = wine[ ,-(12:13)]
X = scale(X, center=TRUE, scale=TRUE)
# Extract the centers and scales from the rescaled data (which are named attributes)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")
# Using kmeans++ initialization
clust_wine = kmeanspp(X, k=2, nstart=25)
```


#### First try: using `residual.sugar` and `alcohol`

First, we randomly select the residuals of the two variables. View cluster membership on the Sugar and Alcohol axes. As shown in the figure, we can hardly see the cluster members using these two variables.

```{r bad cluster, echo=FALSE}
# A few plots with cluster membership shown
# Try some variables not are not gonna look great 
ggplot(wine) + 
  geom_point(aes(residual.sugar, alcohol, color = factor(clust_wine$cluster), shape=factor(clust_wine$cluster)))+
    labs(x ="Residual Sugar", y ="Alcohol ", title = "Cluster Membership")+
          theme(plot.title = element_text(hjust = 0.5, face = "bold"))
```



Unsurprisingly, this way of looking at clusters doesn't help us distinguish between white and red, as we can see in the graph below:

```{r bad_cluster, echo=FALSE}
ggplot(wine) + 
  geom_point(aes(residual.sugar, alcohol, color = color , shape=factor(clust_wine$cluster)))+   scale_color_manual(values=c("red", "grey"))+ guides(shape =guide_legend(title="Cluster"))+
    labs(x ="Residual Sugar", y ="Alcohol ", title = " Is the Clusters Separating White from Red?")+
          theme(plot.title = element_text(hjust = 0.5, face = "bold"))
```

#### Second try: using `volatile.acidity` and `total.sulfur.dioxide`

Now, let's try to choose the volatile of the other two variables. Acidity "and" total sulfur ". Look at the members.
```{r good_cluster, echo=FALSE}
# Hand picked 2 variables to make it look good 
# First see how does our clusters look like on a "" v.s "" map
ggplot(wine) + 
  geom_point(aes(volatile.acidity, total.sulfur.dioxide, color = factor(clust_wine$cluster), shape=factor(clust_wine$cluster)))+
    labs(x ="Volatile Acidity ", y ="Total Sulfur Dioxide ", title = "Cluster Membership")+
          theme(plot.title = element_text(hjust = 0.5, face = "bold"))
```


Now, once again, matching clusters with red/white, we can see that this way of looking at clusters helps us distinguish between white and red.

```{r, echo=FALSE}
## NOW, match the clusters with red/white
ggplot(wine) + 
  geom_point(aes(volatile.acidity, total.sulfur.dioxide, color = color , shape=factor(clust_wine$cluster)))+ scale_color_manual(values=c("red", "grey"))+ guides(shape =guide_legend(title="Cluster"))+
    labs(x ="Volatile Acidity", y ="Total Sulfur Dioxide", title = " Is the Clusters Separating White from Red?")+
          theme(plot.title = element_text(hjust = 0.5, face = "bold"))
```


While this method is good at distinguishing between white and red wines, it is not so good at distinguishing the quality of the wine.

```{r, echo=FALSE}
# not too great at seperating the quality
ggplot(wine) + 
  geom_point(aes(volatile.acidity, total.sulfur.dioxide, color = quality, shape=factor(clust_wine$cluster)))+ 
  guides(shape =guide_legend(title="Cluster"))+
    labs( title = "Quality of the Wine?")+
          theme(plot.title = element_text(hjust = 0.5, face = "bold"))
```




### PCA

Now, we try to run PCA on the data 
```{r PCA, echo=FALSE}
# Now run PCA on the  data
pc_wine = prcomp(X, rank=5, scale=TRUE)
loadings = pc_wine$rotation
scores = pc_wine$x
```


We can simply look at the linear combination of data that defines pc. Each column is a different linear summary of the 11 chemicals.

```{r PCA -2, echo=FALSE}
# these are the linear combinations of data that define the PCs
# each column is a different linear summary of the 11 chemicals 
kable(head(loadings))
```



The five summary features provided us with 80% of the overall variation in the 11 original features. While the compression ratio doesn't look great, it's good enough to tell the difference between red and white.

```{r PCA -3, echo=FALSE}
# 5 summary features gets us 80% of the overall variation in the 11 original features
# although the compression ratio does not look great, it is sufficient to dsitinguish reds and whites 
summary(pc_wine)
qplot(scores[,1], scores[,2], color=wine$color, xlab='Component 1', ylab='Component 2') + scale_color_manual(values=c("red", "grey"))
```


But judging the quality of a wine on a computer is still very difficult.


```{r PCA -4, echo=FALSE}
# The PCA separates the color of the wine well
# but not the quality of the wine
qplot(scores[,1], scores[,3], color=wine$quality, xlab='Component 1', ylab='Component 3')
```

### Conclusion
Using PCA will be more efficient in distinguishing between red and white because we do not need to choose to see clusters of two variables on the map. On the other hand, using PCA allows us to distinguish between red and white by using two principal components. The other thing we can notice is that neither of these unsupervised learning algorithms can distinguish between good wine and bad wine.


## Question 2: Market segmentation

### K-means clustering
```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.width=12, fig.height=12}
set.seed(748737737) 
##### Clustering with K-Means
social = read.csv("social_marketing.csv")
X = social[,-(1)]
X = scale(X, center=TRUE, scale=TRUE)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")
ggcorrplot::ggcorrplot(cor(X), hc.order = TRUE)
```
To make a quick correlation graph, we can see the categories of tweets that are most correlated to each other for a given user.

Next, we'll cluster using K-means to potentially find interesting subsets of Twitter followers based on how often they tweet in certain categories. But first, given that tweets are divided into many different variables, we have to choose the optimal number of clusters.

```{r, echo=FALSE, warning=FALSE, message=FALSE, out.width="50%", out.height="50%"}
#Lets first pick optimal K
k_grid = seq(2, 30, by=1)
SSE_grid = foreach(k = k_grid, .combine='c') %do% {
  cluster_k = kmeans(X, k, nstart=25)
  cluster_k$tot.withinss
}
plot(k_grid, SSE_grid, main = "Elbow Plot")
```


We will choose 11 clusters because it seems to be the closest to the "elbow" point.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# we'll pick k because it seems to be the closest thing to an elbow point
clust1 = kmeans(X, 11, nstart=25)
#clust1$tot.withinss 
#180158.8
## this next section is to categorize everything so I'll put these into an easily readable dataframe
df = data.frame(
  cluster1 = c(clust1$center[1,]*sigma + mu),
  cluster2 = c(clust1$center[2,]*sigma + mu),
  cluster3 = c(clust1$center[3,]*sigma + mu),
  cluster4 = c(clust1$center[4,]*sigma + mu),
  cluster5 = c(clust1$center[5,]*sigma + mu),
  cluster6 = c(clust1$center[6,]*sigma + mu),
  cluster7 = c(clust1$center[7,]*sigma + mu),
  cluster8 = c(clust1$center[8,]*sigma + mu),
  cluster9 = c(clust1$center[9,]*sigma + mu),
  cluster10 = c(clust1$center[10,]*sigma + mu),
  cluster11 = c(clust1$center[11,]*sigma + mu)
)
#transpose for readaability and reshape it
df = data.frame(
  cluster = c(1:11),
  t(df)
)
rownames(df) = 1:nrow(df)
df_melt <- melt(df, id.vars='cluster')
# we'll select the top 5 tweet categories to see what type of user is in each cluster
top_5 = df_melt %>% 
  group_by(cluster) %>%
  arrange(cluster, desc(value)) %>%
  slice_max(order_by = value, n=5)
t1 = top_5 %>% filter(cluster == 1) # College aged gamers
t2 = top_5 %>% filter(cluster == 2) # informed news folks with politics and (cars for some reason. maybe conservatives?...or m e n)
t3 = top_5 %>% filter(cluster == 3) # pornography/nsfw?
t4 = top_5 %>% filter(cluster == 4) # cooking, photo sharing, fashion, beauty, gossip. a type of influencer i think
t5 = top_5 %>% filter(cluster == 5) # dating, talk/gossip. I think influencers/life advice crowd?
t6 = top_5 %>% filter(cluster == 6) # health and fitness crowd
t7 = top_5 %>% filter(cluster == 7) # politics and travel. But I think foreign policy focused folks
t8 = top_5 %>% filter(cluster == 8) # sports, religion, parenting. probably stay at home parents.
t9 = top_5 %>% filter(cluster == 9) # entertainment culture art crowd
t10 = top_5 %>% filter(cluster == 10) # miscellaneous
t11 = top_5 %>% filter(cluster == 11) #my guess is memes. shopping (ads?) photo sharing and chatter
knitr::kables(list(kable(t1[c("variable", "value")], caption = "Cluster 1", digits=3),
                   kable(t2[c("variable", "value")], caption="Cluster 2", digits=3),
                   kable(t3[c("variable", "value")], caption="Cluster 3", digits=3),
                   kable(t4[c("variable", "value")], caption = "Cluster 4", digits=3)
                   )
              )
knitr::kables(list(kable(t5[c("variable", "value")], caption="Cluster 5", digits=3),
                   kable(t6[c("variable", "value")], caption="Cluster 6", digits=3),
                   kable(t7[c("variable", "value")], caption="Cluster 7", digits=3),
                   kable(t8[c("variable", "value")], caption="Cluster 8", digits=3)
                   )
              )
knitr::kables(list(kable(t9[c("variable", "value")], caption="Cluster 9", digits=3),
                   kable(t10[c("variable", "value")], caption="Cluster 10", digits=3),
                   kable(t11[c("variable", "value")], caption="Cluster 11", digits=3)
                   )
              )
```

We will choose 11 clusters because it seems to be the closest to the "elbow" point. Here, we have 11 clusters. We filtered the top 5 categories of tweets for each cluster. Most of us see well-defined groups that seem to make sense. For example: Cluster 9 is a group of college-age video game players. More interestingly, Cluster 11 will be a group of users who primarily post health and fitness tweets.

### PCA

Next, we will try PCA to see if we can find more combinations of variables that can explain users, which cannot be revealed by clustering.

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=13, fig.height=13}
### PCA
pca_social = prcomp(X, rank=11, scale=TRUE)
loadings = pca_social$rotation
head(loadings)
summary(pca_social)
loadings_summary = loadings %>%
  as.data.frame() %>%
  rownames_to_column('Categories')
pc1_ = ggplot(loadings_summary) + geom_col(aes(x=reorder(Categories, PC1), y=PC1)) + coord_flip()
pc2_ = ggplot(loadings_summary) + geom_col(aes(x=reorder(Categories, PC2), y=PC2)) + coord_flip()
pc3_ = ggplot(loadings_summary) + geom_col(aes(x=reorder(Categories, PC3), y=PC3)) + coord_flip()
pc4_ = ggplot(loadings_summary) + geom_col(aes(x=reorder(Categories, PC4), y=PC4)) + coord_flip()
pc5_ = ggplot(loadings_summary) + geom_col(aes(x=reorder(Categories, PC5), y=PC5)) + coord_flip()
pc6_ = ggplot(loadings_summary) + geom_col(aes(x=reorder(Categories, PC6), y=PC6)) + coord_flip()
pc7_ = ggplot(loadings_summary) + geom_col(aes(x=reorder(Categories, PC7), y=PC7)) + coord_flip()
pc8_ = ggplot(loadings_summary) + geom_col(aes(x=reorder(Categories, PC8), y=PC8)) + coord_flip()
pc9_ = ggplot(loadings_summary) + geom_col(aes(x=reorder(Categories, PC9), y=PC9)) + coord_flip()
pc10_ = ggplot(loadings_summary) + geom_col(aes(x=reorder(Categories, PC10), y=PC10)) + coord_flip()
pc11_ = ggplot(loadings_summary) + geom_col(aes(x=reorder(Categories, PC11), y=PC11)) + coord_flip()
grid.arrange(pc1_, pc2_, pc3_, pc4_, pc5_, pc6_, pc7_, pc8_, pc9_, pc10_, pc11_) 
```

Running the PCA shows some of the same user categories as we found with the clustering method, as well as some other user categories, such as music twitter. Interestingly, we also found two subsets of movie/TV tweeters: one is more arts-oriented, and the other is more business/industry oriented. The brand clearly has a diverse following, and it's obvious that they should try to appeal to the culinary or health/fitness crowd. But beverage preferences are so personal that they can run all sorts of targeted ads to appeal to each particular user. Ads can use or avoid certain phrases or words depending on each cluster or major component. For example: ads aimed at the "TV/film and arts" crowd (see PC7) can avoid appealing to online games, cars, or sports.

## Question 3: Association rules for grocery purchases

  Initially, the best option seemed to be to set a relatively high bar for both support and confidence. This approach seems to make sense because support can tell us which rules are worth exploring further. However, when using a minimum support threshold of 0.005 and a confidence level of 0.5, we don't seem to get very impressive results. Simply put, we're pretty sure that people buy whole milk and "other vegetables" when they buy other goods. Given the popularity of milk and vegetables, this is not a very dramatic or interesting result. The maximum item length is set to 10, this is because people usually buy a lot of items at once when shopping and we don't want to miss out on any potentially interesting combinations.

The confidence threshold is set at 0.5, which may seem high, but the higher confidence level is set to counteract the "milk" factor and really extract surprising results. Because milk is such a popular item, many of the rules involving milk and other items have a high degree of credibility, even if the elevation is not very high.

After disappointing results using 0.005 minimum support, we adjusted the minimum support to 0.001 while keeping the confidence and maximum item length unchanged. After extracting the rules, we looked at the rules for promotion > 10, which led to some interesting, but not entirely surprising, associations.

**The 15 rules with lift greater than 10 are listed below:**


```{r 3A, results = FALSE, message=FALSE, echo=FALSE}
#import data, tab delimited text file
raw_groceries <- read.table("groceries.txt", 
                            sep = '\t', header = F)
#add a numbered variable to keep track of baskets
raw_groceries <- raw_groceries %>%
 add_column(Basket_Number= NA) 
raw_groceries$Basket_Number <- 1:nrow(raw_groceries) 
#rename first variable
raw_groceries <- rename(raw_groceries, Customer = V1)
raw_groceries$Basket_Number = factor(raw_groceries$Basket_Number)
#commas into individual items
groceries = strsplit(raw_groceries$Customer, ",")
groceries$Customer = factor(groceries$Customer)
# Remove duplicates ("de-dupe")
# lapply says "apply a function to every element in a list"
# unique says "extract the unique elements" (i.e. remove duplicates)
groceries = lapply(groceries, unique)
grocerytrans = as(groceries, "transactions")
groceryrules = apriori(grocerytrans, 
                     parameter=list(support=.001, confidence=.5, maxlen=10))
grocrules_df <- arules::DATAFRAME(groceryrules)
data_frame_mod <- filter(grocrules_df,lift>10)
sub1 = subset(groceryrules, subset = confidence > 0.01 & support > 0.005)
saveAsGraph(sub1, file = "groceryrules.graphml")
```

```{r 3B, message=FALSE, echo=FALSE}
kable(data_frame_mod[1:15, ], caption = "Rules with lift over 10")
```

**Looking at many of the rules, it's clear that some are compliments such as:**
  
  `{ham, processed cheese} -> white bread`
  
  `{baking powder, flour} -> sugar`
  
  **Other rules might not initially seem like complements, but have clear associations with each other. The rule with the highest lift seems to come from people planning parties or cookouts:**
  
  `{instant food products, soda} -> hamburger meat` 
  
This rule has the highest lift of all the rules we found with 18.998 lift, and may indicate people buying products for cookouts.
  
  `{liquor, red/blush wine} -> bottled beer`
  
  This rule makes sense for parties, it also has a very high confidence of 0.9047619.
  
  `{popcorn, soda} -> salty snack` 
  
  This rule makes sense because people buy these items for parties and movie nights
  
  **Finally, the most amusing rule may be:**
  
  `{Instant food products, whole milk} -> hamburger meat` 
  
 This rule could include people buying the ingredients for the American household staple "burger Helper," which requires instant "burger helper" mixes, milk and burger meat. 
  
### Graphs

Below are some diagrams of the rule set created in the first part of the problem.

Figure 1 takes support and lift as rules, and shadow intensity represents confidence.

Figure 2 shows the support and confidence levels of the rule organization, with different colors for the order of specific rules.

Figure 3 shows a network diagram with > 0.01 confidence and > 0.005 support rules. This is done to make the network more aesthetically pleasing, in an attempt to draw all the rules, creating a confusing and unexplainable network. 
  
```{r 3C, message=FALSE, echo=FALSE}
plot(groceryrules, measure = c("support", "lift"), shading = "confidence")
# "two key" plot: coloring is by size (order) of item set
plot(groceryrules, method='two-key plot')
```

